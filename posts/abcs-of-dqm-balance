---
authors:
  - Pooja Krishnan
tags: null
date: 2020-06-15T14:50:55.000Z
title: 'The ABCs of DQM: Balance'
image: null
---

This post is a part of a series of posts on Data Quality Management. The focus of this series is on strategies for acquiring data that can be used to ensure the quality of data. The five aspects of Data Quality Management are:

1. Accuracy
2. Consistency
3. Completeness
4. Integrity
5. Timeliness

In this article, we will expand on the definition of the ABC Framework as [mentioned in this article](https://blog.ippon.tech/abcs-of-dqm-audit/), written by my colleague [Daniel Ferguson](https://www.linkedin.com/in/daniel-ferguson-985b7048/). There he introduces the ABC Framework of Data Quality Management, particularly focusing on how the Audit process lays the groundwork for high quality data.

In this blog, we will look at the B in the ABC framework, Balance, and how the Balance process on an ETL pipeline can confirm the accuracy and consistency of your data. We'll also examine how the metadata collected as a part of the Audit process can be used to Balance your data.

# How do you Balance your Data?

Balance, to put it simply, is how you can confirm if your ETL processes are operating with the correct data. Consider a situation where you have an ETL process that reads data from a source table's partition (from another system) and writes that partition to a target table within your system. How can you tell that all the data from your source partition made it to your target partition? If you already have an audit process that captures metadata, you can use those metrics to balance your data. Consider the below sample data model for an Audit table created using [SQLDBM](https://app.sqldbm.com/) ![Audit Schema Example](https://raw.githubusercontent.com/ippontech/blog-usa/master/images/2020/07/abcs-dqm-balance-1.png)

With this model, we know how each job ran and we know where those job's records came from and were stored to. This will form the basis for our balance process.

The first step to balancing your data in this scenario might be to look at the `records_written` and `records_read` fields from the Audit table. If they match, all the records you wanted to insert were inserted. This only works if you are inserting every record from your source table to your target. What do you do if there are multiple tables you pull from your source? What if it is difficult to distinguish between a job and a task? Lets look at a sample balance framework to examine what you can do.

## Balancing in Practice

In order to balance your data, you would need to ensure at minimum, the following:

1. That your data in your source table is as expected.
2. The data in your target table is as expected.

![Balance Diagram](https://raw.githubusercontent.com/ippontech/blog-usa/master/images/2020/07/abcs-dqm-balance-2.png)

Consider the above balance framework for an ETL job that retrieves data from a source table (from another system) to a target table in your database. At its simplest form, your balance job could take the form of loading two queries (one against the source dataset and one against the target) and executing them against your two tables. Depending on the ETL process, these queries could be derived from the inputs to the ETL process themselves. Ideally, the queries would capture the following two types of information about your source and target tables:

- Count of Records
- Checks against aggregated values such as amounts

Upon execution of the queries, you would be able to compare the results from the source table to that from the target table. If they match, your data is balanced. If there are discrepancies, however, any errors need to feed directly into the Control Process. After all, it is in the Control Process, that our ABC framework can make corrections to the data to ensure consistency from the start of ETL execution to its completion.

> What I describe above would occur **only** after successful execution of your ETL job. If the ETL job fails partway or encounters an exception, however, it would be the Balance job that reverts any changes and returns both environments to their prior state.

# A Decoupled Approach to ETL and Balance

Events allow you to asynchronously transmit data from one application to another. Your data quality jobs can be triggered by notification or event upon completion of your ETL job or jobs. One of these would ideally be your balance job. In an event-driven ETL process, balancing your data is a little trickier than a traditional ETL process, as it is more difficult to distinguish between a job and a task. A platform that processes credit card transactions, for example, would be processing each transaction as a separate event. There are usually a series of processes a bank or credit card company use to verify transactions and eventually post them to your account. How would you balance such a system?

## An Aside on Orchestration in Event-Driven ETL

In the [Balancing in Practice](#balancing-in-practice) section above, we looked at a balance architecture that ran serially; upon completion of our ETL Job, we ran the balance process, followed by control. In an event-driven ETL pipeline, the entire ETL execution could mean processing thousands or millions of events. Thus we would have to run balance and control while we are completing ETL Job execution. ![Orchestration of DQM](https://raw.githubusercontent.com/ippontech/blog-usa/master/images/2020/07/abcs-dqm-balance-3.png)

## Balancing the Event-Driven Way

When balancing the event-driven way, we first need to define what an event looks like for the ABC Process as a whole. Events for your DQM Process will mainly be capturing the metadata created by your ETL Job. Some of the metadata that can be included in the event are:

- Primary Keys
- Data Types
- Row Counts
- Fields to audit for quality
- Fields to balance for quality
- Queries to be executed for auditing or balancing your data

With this understanding of what an event looks like, we can now examine what balancing could look like. ![Event-Driven Balance](https://raw.githubusercontent.com/ippontech/blog-usa/master/images/2020/07/abcs-dqm-balance-4.png) In the diagram above, I've simplified the event which would be the input to the balance process to three specific types of metadata: primary keys, queries to be executed and row counts. For each event you would want to check that all primary key constraints have been upheld to ensure data integrity. You would also want to check the row counts and execute any queries for balancing data. If all the constraints match up, much like the case we discussed above, your data is balanced. However, if there are any discrepancies, as a part of the balance job, the response would be different. In the balance process for a batch ETL job, all the results are generated at once and can be fed into the control process. For an event-driven balance process, however, for each event that encounters discrepancies as a part of the balance process, the results and even specific rows from the dataset could be sent to the control process. From here, it would again be the realm of the control process, which will be addressed in an upcoming post, to correct the data.
